# Негативные последствия паттернов надежности и масштабирования
Все приведенные негативные эффекты взяты из личного опыта.

## Cache
Представим, что есть корневой сервис, написанный на **Python**. В нем реализуется вся основная бизнес логика, влияющая на 
состояние базы данных. Пользователь не ходит напрямую в этот сервис, а ходит в прокси-сервис на **С++**. 
На каждый запрос пользователя для получения данных, прокси-сервис так же не ходит напрямую в корневой сервис, а ходит 
в свой кэш, в котором хранит **diff** данных. За эти **diff**'ом прокси-сервис ходит в корневой сервис, условно, раз в 5 минут и
обновляет его. Пользовательская библиотека, реализующая интерфейс к прокси-сервису, умеет применять этот **diff** 
и обновлять объект. Проблема возникает, когда есть несколько инстансов прокси-сервиса и в результате работы балансировщика нагрузки,
два последовательных запроса пользователя могут прийти на разные инстансы прокси-сервиса, в которых, теоретически, может быть разный кэш, 
т. к. где-то по какой-то причине он не обновился, например, из-за моргнувшей сети. Чтобы этого избежать, вероятно, 
необходимо придумывать какой-то гибкий алгоритм когерентности кэшей инстансов. Что-то аналогично тому, как устроен [механизм когерентности 
кэшей процессора](https://ru.wikipedia.org/wiki/Когерентность_кэша). На практике такая ситуация происходила не часто и редко
приводила к инцидентам. Однако с учетом органического роста трафика, думаю, что проблема будет становиться все актуальнее.


## Rate Limiting
Представим, что в наш сервис ходят N клиентов, есть дефолтный **rate-limit** для всех клиентов, есть так же возможность гибко
задавать объем для какого-то конкретного эндпоинта клиента, с которого в наш сервис посылается запрос. 
На практике может возникать ситуация, что у клиента случается органический рост и ему теперь необходимо иметь возможность посылать в 
нас бОльший объем трафика, а сделать этого он не может, т. к. до послабления ограничений с нашей стороны, 
запросы, превышающие объем, будут отклоняться. Мы, как принимающая сторона, можем и не знать об этой потребности клиента, что по итогу
может вылиться в какие-то финансовые и репутационные издержки.


## Queuing
Представим, что наше сообщение по какой-то причине не смогло обработаться consumer'ом, тогда оно может быть положено в конец очереди.
Затем, когда очередь снова дойдет до нашего сообщения, оно уже может быть успешно обработано, но принимающая сторона более не ожидает его
(или ожидало в другое время). Так же пишущая сторона может вновь задублировать сообщение, что так же может быть непонято принимающей стороной.
Все это может привести к сложному и долгому дебагу. 